{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 From Time Series to Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from kats.consts import TimeSeriesData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>revenue</th>\n",
       "      <th>stock</th>\n",
       "      <th>price</th>\n",
       "      <th>promo_type_1</th>\n",
       "      <th>promo_bin_1</th>\n",
       "      <th>promo_type_2</th>\n",
       "      <th>...</th>\n",
       "      <th>product_width</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>hierarchy1_id</th>\n",
       "      <th>hierarchy2_id</th>\n",
       "      <th>hierarchy3_id</th>\n",
       "      <th>hierarchy4_id</th>\n",
       "      <th>hierarchy5_id</th>\n",
       "      <th>storetype_id</th>\n",
       "      <th>store_size</th>\n",
       "      <th>city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>S0002</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>PR14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR03</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>cluster_5</td>\n",
       "      <td>H01</td>\n",
       "      <td>H0105</td>\n",
       "      <td>H010501</td>\n",
       "      <td>H01050100</td>\n",
       "      <td>H0105010006</td>\n",
       "      <td>ST04</td>\n",
       "      <td>39</td>\n",
       "      <td>C007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0001</td>\n",
       "      <td>S0012</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>PR14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR03</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>cluster_5</td>\n",
       "      <td>H01</td>\n",
       "      <td>H0105</td>\n",
       "      <td>H010501</td>\n",
       "      <td>H01050100</td>\n",
       "      <td>H0105010006</td>\n",
       "      <td>ST04</td>\n",
       "      <td>28</td>\n",
       "      <td>C005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0001</td>\n",
       "      <td>S0013</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>PR14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR03</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>cluster_5</td>\n",
       "      <td>H01</td>\n",
       "      <td>H0105</td>\n",
       "      <td>H010501</td>\n",
       "      <td>H01050100</td>\n",
       "      <td>H0105010006</td>\n",
       "      <td>ST04</td>\n",
       "      <td>33</td>\n",
       "      <td>C026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0001</td>\n",
       "      <td>S0023</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>PR14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR03</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>cluster_5</td>\n",
       "      <td>H01</td>\n",
       "      <td>H0105</td>\n",
       "      <td>H010501</td>\n",
       "      <td>H01050100</td>\n",
       "      <td>H0105010006</td>\n",
       "      <td>ST04</td>\n",
       "      <td>31</td>\n",
       "      <td>C008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0001</td>\n",
       "      <td>S0025</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>PR14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PR03</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>cluster_5</td>\n",
       "      <td>H01</td>\n",
       "      <td>H0105</td>\n",
       "      <td>H010501</td>\n",
       "      <td>H01050100</td>\n",
       "      <td>H0105010006</td>\n",
       "      <td>ST04</td>\n",
       "      <td>25</td>\n",
       "      <td>C024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id store_id       date  sales  revenue  stock  price promo_type_1  \\\n",
       "0      P0001    S0002 2017-01-02    0.0     0.00    8.0   6.25         PR14   \n",
       "1      P0001    S0012 2017-01-02    1.0     5.30    0.0   6.25         PR14   \n",
       "2      P0001    S0013 2017-01-02    2.0    10.59    0.0   6.25         PR14   \n",
       "3      P0001    S0023 2017-01-02    0.0     0.00    6.0   6.25         PR14   \n",
       "4      P0001    S0025 2017-01-02    0.0     0.00    1.0   6.25         PR14   \n",
       "\n",
       "  promo_bin_1 promo_type_2  ... product_width cluster_id hierarchy1_id  \\\n",
       "0         NaN         PR03  ...          20.0  cluster_5           H01   \n",
       "1         NaN         PR03  ...          20.0  cluster_5           H01   \n",
       "2         NaN         PR03  ...          20.0  cluster_5           H01   \n",
       "3         NaN         PR03  ...          20.0  cluster_5           H01   \n",
       "4         NaN         PR03  ...          20.0  cluster_5           H01   \n",
       "\n",
       "   hierarchy2_id  hierarchy3_id  hierarchy4_id hierarchy5_id storetype_id  \\\n",
       "0          H0105        H010501      H01050100   H0105010006         ST04   \n",
       "1          H0105        H010501      H01050100   H0105010006         ST04   \n",
       "2          H0105        H010501      H01050100   H0105010006         ST04   \n",
       "3          H0105        H010501      H01050100   H0105010006         ST04   \n",
       "4          H0105        H010501      H01050100   H0105010006         ST04   \n",
       "\n",
       "  store_size city_id  \n",
       "0         39    C007  \n",
       "1         28    C005  \n",
       "2         33    C026  \n",
       "3         31    C008  \n",
       "4         25    C024  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stores\n",
    "df_stores = pd.read_csv('../dataset/store_cities.csv', \n",
    "                        delimiter=',', \n",
    "                        dtype={'store_id':'category',\n",
    "                               'storetype_id':'category',\n",
    "                               'city_id':'category'})\n",
    "# Product hierachy\n",
    "df_product_hierachy = pd.read_csv('../dataset/product_hierarchy.csv', \n",
    "                                 delimiter= ',',\n",
    "                                 dtype={'product_id':'category',\n",
    "                                        'cluster_id':'category',\n",
    "                                        'hierarchy1_id':'category',\n",
    "                                        'hierarchy2_id':'category',\n",
    "                                        'hierarchy3_id':'category',\n",
    "                                        'hierarchy4_id':'category',\n",
    "                                        'hierarchy5_id':'category'})\n",
    "# Sales\n",
    "df_sales = pd.read_csv('../dataset/sales.csv',\n",
    "                       delimiter=',', \n",
    "                       dtype={\"product_id\":\"category\", \n",
    "                              \"store_id\":\"category\",\n",
    "                              \"promo_type_1\":\"category\",\n",
    "                              \"promo_bin_1\":\"category\",\n",
    "                              \"promo_type_2\":\"category\",\n",
    "                              \"promo_bin_2\":\"category\",\n",
    "                              \"promo_discount_2\":\"category\",\n",
    "                              \"promo_discount_type_2\":\"category\"},\n",
    "                       parse_dates=[\"date\"])\n",
    "\n",
    "# Join hierachy and sales\n",
    "df_sales = df_sales.join(df_product_hierachy.set_index('product_id'), on='product_id')\n",
    "df_sales = df_sales.join(df_stores.set_index('store_id'), on='store_id')\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATS\n",
    "Kats (Kits to Analyze Time Series) is a light-weight, easy-to-use, extenable, and generalizable framework to perform time series analysis in Python. Time series analysis is an essential component of data science and engineering work. Kats aims to provide a one-stop shop for techniques for univariate and multivariate time series including:\n",
    "\n",
    "1. Forecasting\n",
    "2. Anomaly and Change Point Detection\n",
    "3. Feature Extraction\n",
    "\n",
    "Have a look at the example notebooks at [GitHub](https://github.com/facebookresearch/Kats/tree/main/tutorials)\n",
    "We will look only at parts of the tutorials here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kats Basics\n",
    "`TimeSeriesData` is the basic data structure in Kats to represented univariate and multivariate time series.  There are two ways to initiate it, henceforth referred to as \"Method 1\" and \"Method 2\":\n",
    "\n",
    "1) `TimeSeriesData(df)`, where `df` is a `pd.DataFrame` object with a \"time\" column and any number of value columns.\n",
    "\n",
    "2) `TimeSeriesData(time, value)`, where `time` is either a `pd.Series` or `pd.DatetimeIndex` object and `value` is either a `pd.Series` (for univariate) or a `pd.DataFrame` (for multivariate)\n",
    "\n",
    "\n",
    "We will use a slice of our `sales` dataset  to demonstrate how to create a `TimeSeriesData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's aggregate e. g. the number of sales for each hierarchy1 level per day. \n",
    "df_global_sales_by_hierarchy1 = df_sales.groupby(by=['date', 'hierarchy1_id'])['sales'].sum()\n",
    "\n",
    "# Get a slice for a specific hierachy level\n",
    "ts_sales_h01 = df_global_sales_by_hierarchy1.loc['2017-01-01':'2019-10-01', 'H01']\n",
    "\n",
    "# TimeSeriesData requires \n",
    "ts_sales_h01 = ts_sales_h01.to_frame().reset_index()\n",
    "ts_sales_h01.rename(columns={'date':'time', 'sales':'value'}, inplace=True)\n",
    "#display(df_global_sales_by_hierarchy1.head(), df_global_sales_by_hierarchy1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sales_h01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeriesData(time=ts_sales_h01.time, value=ts_sales_h01.value)\n",
    "ts.plot(cols=['value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with Kats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kats currently support the following 10 base forecasting models: \n",
    "\n",
    "1. Linear  \n",
    "2. Quadratic   \n",
    "3. ARIMA   \n",
    "4. SARIMA   \n",
    "5. Holt-Winters   \n",
    "6. Prophet   \n",
    "7. AR-Net   \n",
    "8. LSTM   \n",
    "9. Theta   \n",
    "10. VAR   \n",
    "\n",
    "Each models follows the `sklearn` model API pattern:  we create an instance of the model class and then call its `fit` and `predict` methods.  In this section, we provide examples for the Prophet and Theta models.  A more in-depth introduction to forecasting in Kats is provided in [kats_201_forecasting.ipynb](https://github.com/facebookresearch/Kats/blob/main/tutorials/kats_201_forecasting.ipynb).\n",
    "\n",
    "We will use Prophet as an example on our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the param and model classes for Prophet model\n",
    "from kats.models.prophet import ProphetModel, ProphetParams\n",
    "\n",
    "# create a model param instance\n",
    "params = ProphetParams(seasonality_mode='multiplicative') # additive mode gives worse results\n",
    "\n",
    "# create a prophet model instance\n",
    "m = ProphetModel(ts, params)\n",
    "\n",
    "# fit model simply by calling m.fit()\n",
    "m.fit()\n",
    "\n",
    "# make prediction for next 30 month\n",
    "fcst = m.predict(steps=30, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the predict method returns a dataframe as follows\n",
    "fcst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results with uncertainty intervals\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection with Kats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kats provides a set of models and algorithms to detect outliers, change points, and trend changes in time series data.\n",
    "\n",
    "\n",
    "### Available algorithms\n",
    "\n",
    "To detect a specific pattern, we provided different algorithms, which is summarized as follows.\n",
    "- **Outlier Detection**. This usually refers to a abnormal spike in a time series data, which can be detected with `OutlierDetector`\n",
    "- **Change Point Detection**. This refers to a sudden change that the time series have different statistical properties before and after the change. We provided three major algorithms to detect such patterns:\n",
    "    - CUSUM Detection\n",
    "    - Bayesian Online Change Point Detection (BOCPD)\n",
    "    - Stat Sig Detection\n",
    "- **Trend Change Detection**. This refers to a slow trend change on the time series data, which can be detected with Mann-Kendall detection algorithm, `MKDetector`\n",
    "\n",
    "In this tutorial, we will demonstrate the usage of the `OutlierDetector`. For a more in-depth introduction to detection in Kats look at this notebeook: [kats_202_detection.ipynb](https://github.com/facebookresearch/Kats/blob/main/tutorials/kats_202_detection.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example with outlier detection method\n",
    "\n",
    "We use the `OutlierDetector` module to detect outliers in time series.  \n",
    "\n",
    "The outlier detection algorithm works as follows:\n",
    "\n",
    "- Do a [seasonal decomposition](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) of the input time series with additive or multiplicative decomposition as specified (default is additive)\n",
    "- Generate a residual time series by either removing only trend or both trend and seasonality if the seasonality is strong.\n",
    "- Detect points in the residual which are outside 3 times the inter quartile range.  This multiplier can be tuned using the `iqr_mult` parameter in `OutlierDetector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.outlier import OutlierDetector\n",
    "\n",
    "# We use pd.IndexSlice to acces elements in our hierachical data structure\n",
    "idx = pd.IndexSlice \n",
    "    \n",
    "ts = (df_global_sales_by_hierarchy1\n",
    "         .to_frame()                    # KATS want's a dataframe, so it gets one\n",
    "         .loc[idx[:, 'H01', :]]      # We are slicing based on the hierachy\n",
    "         .reset_index()                 # Our DataFrame has a DateTime index, KATS needs that as a column calld time, so we reindex\n",
    "         .rename(columns={'date':'time', 'sales':'value'})) # and rename our columns\n",
    "\n",
    "# Transorm it to a TimeSeriesData Object\n",
    "ts = TimeSeriesData(ts)\n",
    "ts_outlierDetection = OutlierDetector(ts, 'additive') # call OutlierDetector\n",
    "ts_outlierDetection.detector() # apply OutlierDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the outliers that the algorithum found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = ts_outlierDetection.outliers[0]\n",
    "print(f'Number of outliers: {len(outliers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the outliers together with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15,5))\n",
    "\n",
    "ax.plot(ts_sales_h01.set_index('time')['value'], label = 'y' )\n",
    "ax.plot(ts_sales_h01.set_index('time').loc[outliers, 'value'], linewidth=0, marker='x', color='r', label ='outlier')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(f'H01 Outlier:{len(outliers)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example using a prediction model\n",
    "\n",
    "We use the `Prophet` to build a prediction model for every hierachy1 level timeseries  \n",
    "\n",
    "The outlier detection algorithm works as follows:\n",
    "\n",
    "- Train a prediction model for every single time series\n",
    "- Calculate the insample confidence intervalls for every single time series\n",
    "- Detect points that exceed the confidence intervalls as outliers for every single time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function takes a trained prophet model\n",
    "# and marks insample outliers, when the exceed the model's\n",
    "# confidence intervalls \n",
    "from sklearn.metrics import mean_squared_error\n",
    "def plot_prophet_insample_anononalies(model:ProphetModel, title=None):\n",
    "    # Just do an insample prediction\n",
    "    future = model.model.make_future_dataframe(periods=0)\n",
    "    forecast = model.model.predict(future)\n",
    "\n",
    "    # Joint the history and the prediction DataFrames\n",
    "    df_results = model.model.history.join(forecast.set_index('ds'), \n",
    "                                    on='ds')[['ds', \n",
    "                                              'y',\n",
    "                                              'yhat', \n",
    "                                              'yhat_lower',\n",
    "                                              'yhat_upper']]\n",
    "    df_results = df_results.set_index('ds')\n",
    "    \n",
    "    # Flag values as anonomaly if they exceed the confidence intervalls\n",
    "    outliers = df_results[(df_results.y > df_results.yhat_upper) |\n",
    "              (df_results.y < df_results.yhat_lower)]['y']\n",
    "\n",
    "    # Calc the RMSE to validate the overall prediction performance\n",
    "    rmse = mean_squared_error(df_results['y'], df_results['yhat'],squared=False)\n",
    "\n",
    "    # Plot that stuff\n",
    "    fig, ax = plt.subplots(1,1,figsize=(15,3))\n",
    "    ax.fill_between(df_results.index, \n",
    "                    df_results.yhat_lower, \n",
    "                    df_results.yhat_upper, \n",
    "                    alpha=0.5,\n",
    "                    color='grey',\n",
    "                    label='confidence interval')\n",
    "                                              \n",
    "    ax.plot(df_results.yhat, \n",
    "            label = '$\\hat{y}$')\n",
    "                                              \n",
    "    ax.plot(df_results.y, \n",
    "            linewidth=0, \n",
    "            marker='.',\n",
    "            color='black',\n",
    "            label='y')\n",
    "                                              \n",
    "    ax.plot(outliers, \n",
    "            linewidth=0, \n",
    "            marker='x', \n",
    "            color ='red',  \n",
    "            label = 'outlier')\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(f'{title} Insample RMSE={rmse:.2f}, Outlier:{len(outliers)}')\n",
    "                                              \n",
    "    ax.legend(ncol=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.models.prophet import ProphetModel, ProphetParams\n",
    "\n",
    "# We fit a Prophet model for each hierachy1 level\n",
    "\n",
    "gs_models_l1 = {}\n",
    "for hierachy in df_product_hierachy.hierarchy1_id.cat.categories:\n",
    "    \n",
    "    # We use pd.IndexSlice to acces elements in our hierachical data structure\n",
    "    idx = pd.IndexSlice \n",
    "    \n",
    "\n",
    "    # Prepare the data for prophet, don't get confused we do it step by step\n",
    "    ts = (df_global_sales_by_hierarchy1\n",
    "         .to_frame()                    # KATS want's a dataframe, so it gets one\n",
    "         .loc[idx[:, hierachy, :]]      # We are slicing based on the hierachy\n",
    "         .reset_index()                 # Our DataFrame has a DateTime index, KATS needs that as a column calld time, so we reindex\n",
    "         .rename(columns={'date':'time', 'sales':'value'})) # and rename our columns\n",
    "    \n",
    "    # convert it to KATS TimeSeries object\n",
    "    ts = TimeSeriesData(ts) \n",
    "\n",
    "    # create a model param instance\n",
    "    params = ProphetParams(interval_width=0.95) # additive mode gives worse results\n",
    "\n",
    "    # create a prophet model instance\n",
    "    model = ProphetModel(ts, params)\n",
    "\n",
    "    # fit model simply by calling m.fit()\n",
    "    model.fit()\n",
    "    \n",
    "    gs_models_l1[hierachy] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what we got with this approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in gs_models_l1:\n",
    "    plot_prophet_insample_anononalies(gs_models_l1[key], key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends and seasonalities\n",
    "Most of the Kats models, do a  [seasonal decomposition](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) of the input time series with additive or multiplicative decomposition as specified (default is additive).\n",
    "We can also make use of that to explore trends and seasonalities in our time series:\n",
    "\n",
    "We see:\n",
    "\n",
    "**Trend**\n",
    "- Our time series has an increasing trend since `2017` until ~ `05-2018`. \n",
    "- After that it keeps the level until `04-2019` and is \n",
    "- decreasing after that until the end. \n",
    "\n",
    "**Seasonality weekly**\n",
    "- Saturdays and Sundays are the strongest days\n",
    "- Tuesday until Friday there is a lower acitivity\n",
    "\n",
    "**Seasonality yearly**\n",
    "- In a yearly cycle we have peaks around August and Oktober\n",
    "- November until January have a low volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take our forecast from H01 as an example\n",
    "\n",
    "model = gs_models_l1['H01']\n",
    "\n",
    "future = model.model.make_future_dataframe(periods=0)\n",
    "forecast = model.model.predict(future)\n",
    "\n",
    "# Plot the single components of our prophet model\n",
    "model.model.plot_components(forecast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting time windows with increasing/decreasing sales amounts\n",
    "for a more detailed explanation have a look into the KATS jupyter example notebook on detection [kats_202_detection.ipynb](https://github.com/facebookresearch/Kats/blob/main/tutorials/kats_202_detection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for prophet, don't get confused we do it step by step\n",
    "ts = (df_global_sales_by_hierarchy1\n",
    "     .to_frame()                    # KATS want's a dataframe, so it gets one\n",
    "     .loc[idx[:, 'H01', :]]      # We are slicing based on the hierachy\n",
    "     .reset_index()                 # Our DataFrame has a DateTime index, KATS needs that as a column calld time, so we reindex\n",
    "     .rename(columns={'date':'time', 'sales':'value'})) # and rename our columns\n",
    "ts = TimeSeriesData(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MKDetector` is the trend detection algorithm that we include in Kats, which is based on the non-parametric Mann-Kendall (MK) Test. What the`MKDetector essentially does is apply a MK to a window of fied size (specified by the `window_size argument` in the`detector` method) and return the end point of each window for which this test is statistically significant. Trend windows are detected based on the monotonicity of the increases or decreases in the time series in the window, not the magnitude of the change in the value of the time series over the window.\n",
    "\n",
    "For this example we want to detect time windows, where a decreasing trend persisted for minimum 21 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.detectors.trend_mk import MKDetector\n",
    "\n",
    "detector = MKDetector(data=ts, threshold=.8)\n",
    "# run detector\n",
    "detected_time_points = detector.detector(direction='down', window_size=21)\n",
    "# plot the results\n",
    "detector.plot(detected_time_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return type of the detector method is `List[Tuple[TimeSeriesChangePoint, MKMetadata]`. Each `TimeSeriesChangePoin` returned is the end of an increasing or decreasing trend window of duration `window_size`. In our example, we are looking for decreasing trend windows of length 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp, meta = detected_time_points[0]\n",
    "cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is resposible for the outliers in H01 at `2019-08-16` and `2019-08-17` ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all sales for hierachy1_id and look at the sales in the second level.\n",
    "df_global_sales_h01 = df_sales[df_sales.hierarchy1_id=='H01'][['date', 'sales', 'store_id','hierarchy2_id']]\n",
    "df_global_sales_h01_by_h2 = df_global_sales_h01.groupby(['date', 'hierarchy2_id'])['sales'].sum()\n",
    "df_global_sales_h01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit again models for every hierachy on level 2\n",
    "gs_models_l2 = {}\n",
    "for hierachy in df_global_sales_h01.hierarchy2_id.unique():\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    # Prepare the data for prophet, don't get confused we do it step by step\n",
    "    ts = (df_global_sales_h01_by_h2\n",
    "         .to_frame()                    # KATS want's a dataframe, so it gets one\n",
    "         .loc[idx[:, hierachy, :]]      # We are slicing based on the hierachy\n",
    "         .reset_index()                 # Our DataFrame has a DateTime index, KATS needs that as a column calld time, so we reindex\n",
    "         .rename(columns={'date':'time', 'sales':'value'})) # and rename our columns\n",
    "    \n",
    "    # convert it to KATS TimeSeries object\n",
    "    ts = TimeSeriesData(ts) \n",
    "\n",
    "    # create a model param instance\n",
    "    params = ProphetParams(interval_width=0.95) # additive mode gives worse results\n",
    "\n",
    "    # create a prophet model instance\n",
    "    model = ProphetModel(ts, params)\n",
    "\n",
    "    # fit model simply by calling m.fit()\n",
    "    model.fit()\n",
    "    \n",
    "    gs_models_l2[hierachy] = model\n",
    "    \n",
    "    \n",
    "# Let's see what we've got\n",
    "\n",
    "for key in gs_models_l2:\n",
    "    plot_prophet_insample_anononalies(gs_models_l2[key], key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Insight: \n",
    "Anomaly! The sales at `2019-08-17`, `2019-08-17` in Category H01 where by #% higher than expected. This was due to an increase in sales of Type H0107 by #%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about trends?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Questions (just some brainstorming ideas):\n",
    "- Is this a global trend, or does it only apply to a specific store/city?\n",
    "- How does this affect the revenue and the material stock?\n",
    "- Are there trend changes in the dataset?\n",
    "- Are the sales affected by any promo codes?\n",
    "- Are there repeating sub-series?\n",
    "- Can you idetify clusters in anomalies across different product hierachies/stores\n",
    "- Show us, what you find in the dataset and tell us a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
